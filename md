
# Autonomous Surgical Robotics AI Training Pipeline  
**Prototype v19.0** â€“ ACT-Inspired Chunked Transformer + Hierarchical Task Conditioning + CVAE + NVIDIA Warp + Digital Twins

**Status:** Research prototype â€“ simulation-heavy, early-stage sim2real bridging  
**Date of this snapshot:** February 2026  
**License:** (not specified â€“ assume research/academic use only)

## Overview

This repository contains an **end-to-end research prototype** for training **vision-language-action** policies for **autonomous da Vinci-style surgical robots**, with strong emphasis on:

- soft-tissue simulation realism  
- stochastic action generation (CVAE)  
- hierarchical & chunked action prediction (inspired by ACT / Diffusion Forcing)  
- offline BC â†’ DAgger loop  
- digital twin integration from real CT/MRI scans  
- sim-to-real gap analysis

Current focus: **suturing, cutting, grasping, tissue manipulation** tasks using replayed JIGSAWS teleoperation data + synthetic Warp-based simulation.

## Key Technical Features (v19 highlights)

- **Physics** â†’ NVIDIA Warp (XPBD solver) â€“ GPU-accelerated, differentiable soft bodies  
- **Digital Twins** â†’ patient-specific meshes from NIfTI (CT/MRI) via marching cubes  
- **Blood flow** â†’ simple particle advection + bleeding triggers  
- **Variable tissue properties** â†’ diseased vs. healthy elasticity randomization  
- **Stochastic policy** â†’ Conditional Variational Autoencoder (CVAE) for action chunks  
- **Observation** â†’ multi-modal: joint states (76-dim) + RGB-D + segmentation (8 ch)  
- **Action space** â†’ position deltas (6DoF Ã— 2 arms) + gripper + tool swap  
- **Temporal modeling** â†’ history of 5 steps + predict next 10-step action chunk  
- **Training** â†’ Behavior Cloning â†’ DAgger â†’ KL-regularized CVAE loss  
- **Evaluation** â†’ multi-seed sim success rate + reward + haptic violation metrics  
- **Deployment stub** â†’ ROS 2 node with haptic force visualization  

## Architecture

```
Data Sources
â”œâ”€ JIGSAWS teleop kinematics & video (real expert)
â””â”€ Warp simulation (synthetic augmentation + curriculum)

â†“ (offline collection + DAgger)

SurgicalDataset
â”œâ”€ History vector (T=5 Ã— 76)
â”œâ”€ History images (T=5 Ã— 224Ã—224Ã—8)
â”œâ”€ Future action chunk (T=10 Ã— action_dim)
â””â”€ Task ID embedding

â†“

CVAE-ACT Model
â”œâ”€ Vision: ConvNeXt backbone â†’ 384-dim emb per frame
â”œâ”€ Vector state â†’ Linear projection
â”œâ”€ Concat + Task embedding + PosEnc
â”œâ”€ TransformerEncoder (6 layers)
â”œâ”€ CVAE head: Î¼, logvar â†’ z ~ N(Î¼,ÏƒÂ²)
â””â”€ Decoder: z + task â†’ 10-step action chunk

â†“ (MSE + smoothness + KL)

Optimizer: AdamW Â· LR 3e-5 Â· grad clip 1.5

â†“ (DAgger loop)

Policy rollouts in Warp env â†’ expert correction â†’ aggregate

â†“

Evaluation: success rate, avg reward, force/penetration violation

â†“ (optional)

ROS 2 inference node â†’ Warp sim + haptic viz
```

## Requirements

- Python 3.10â€“3.12  
- PyTorch 2.1+ (CUDA 12.x recommended)  
- NVIDIA Warp (`pip install warp-lang`)  
- `nibabel`, `scikit-image`, `torchvision`, `pandas`, `opencv-python`  
- ROS 2 Humble / Iron (for deployment node)  
- (optional) JIGSAWS dataset (~20â€“30 GB)

```bash
# Minimal working set (Ubuntu 22.04/24.04 example)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install warp-lang nibabel scikit-image pandas opencv-python matplotlib rclpy
```

## Installation & Quick Start

1. Clone repo
```bash
git clone https://github.com/your-org/autonomous-surgical-robotics.git
cd autonomous-surgical-robotics
```

2. Install dependencies (see above)

3. Prepare data
   - Place JIGSAWS kinematics & video files under `./jigsaws_data/`
   - (optional) Add sample NIfTI file as `patient_ct.nii`

4. Train from real JIGSAWS data
```bash
python main.py   # runs acquire â†’ train â†’ DAgger â†’ eval â†’ save
```

Outputs:
- `surgical_model_v18.pth` (or v19)
- `norms_v18.pth`
- `losses_v18.png`
- `logs_v18.json`

5. Visualize simulation (GUI mode)
```python
env = WarpSurgicalEnv(gui=True)
```

6. Run ROS 2 inference loop
```bash
ros2 run <your_package> surgical_node_v18
```

## Current Weaknesses & Known Limitations

| Area                     | Issue                                                                 | Severity |
|--------------------------|-----------------------------------------------------------------------|----------|
| Action application       | Very simplistic joint control (no proper IK / OSC)                    | â˜…â˜…â˜…â˜…     |
| Cutting & suturing       | Extremely crude geometric primitives â€“ no real topology change       | â˜…â˜…â˜…â˜…â˜…    |
| Thread / needle physics  | Basic spring chain â€“ no real needle threading or knotting             | â˜…â˜…â˜…â˜…     |
| Digital twin             | Marching cubes surface mesh only â€“ no good tet mesh yet               | â˜…â˜…â˜…â˜…     |
| Sim2real gap             | No domain randomization for camera / lighting / latency               | â˜…â˜…â˜…      |
| Tool changing            | Stub â€“ no geometry or collision change                                 | â˜…â˜…â˜…      |
| Haptic rendering         | Very approximate â€“ no proper tool-tissue force model                   | â˜…â˜…â˜…      |
| Scalability              | Full Warp sim inside training loop is slow even on high-end GPU       | â˜…â˜…â˜…      |
| Task diversity           | Only 5 coarse tasks â€“ no fine-grained gesture parsing                  | â˜…â˜…       |

## Suggested Improvements (Shortâ€“Medium Term)

1. Replace manual joint stepping with proper **Operational Space Control** (OSC) or differential IK  
2. Integrate **ARCSim / SOFA / MuJoCo-FEM** hybrid for more realistic cutting & suturing  
3. Use **tetgen / PyTetWild** to generate proper tetrahedral meshes from digital twins  
4. Implement **domain randomization** pipeline (camera intrinsics, lighting, specular, blood amount, breathing motion)  
5. Replace chunk prediction with **diffusion policy** or **autoregressive tokenization + LLM-style decoding**  
6. Add **force/torque prediction head** and train with haptic augmentation loss  
7. Port evaluation to **real da Vinci Research Kit (dVRK)** or **da Vinci Si/Xi** via ROS bridge  
8. Record **multi-view RGB-D** streams and fuse in model (stereo + overhead)  
9. Implement **online RL fine-tuning** loop (PPO / DrQ-v2 style) using Warp gradients  

## Longer-term Future Directions

- Full topology-aware tissue cutting & needle threading  
- Multi-task hierarchical policy with LLM-style task decomposition  
- Foundation model pre-training on large-scale surgical video datasets  
- Zero-shot generalization to unseen anatomies via digital twin + language  
- Closed-loop autonomous suturing on physical phantom / cadaver  
- Regulatory-grade sim2real validation pipeline (FDA / ISO 13485 track)

## Citation

If you use ideas or code from this prototype in your research, please consider citing:

```bibtex
@misc{surgical-robotics-prototype-2026,
  author       = {Your Name / Team},
  title        = {Autonomous Surgical Robotics â€“ Warp + CVAE + Digital Twins Prototype},
  year         = {2026},
  note         = {Research prototype â€“ v19.0}
}
```

## Contributing

This is currently a solo / small-team research prototype.

Welcomed: bug reports, sim realism suggestions, better cutting/suturing ideas, digital-twin meshing help.

Pull requests welcome â€“ especially in the following areas:

- realistic soft-body primitives  
- better action representation / control  
- domain randomization suite  
- real hardware integration stubs

---

**Happy hacking â€“ and stay precise!** âœ‚ï¸ğŸ§µ
```

Feel free to modify author names, license, repository link, version numbering, or add badges (Python version, CUDA, arXiv link, etc.).

